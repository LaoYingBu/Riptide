{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import tensorflow.keras.layers as nn\n",
    "\n",
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.custom_gradient\n",
    "def AlphaClip(x, alpha):\n",
    "    output = tf.clip_by_value(x, 0, alpha)\n",
    "\n",
    "    def grad_fn(dy):\n",
    "        x_grad_mask = tf.cast(tf.logical_and(x >= 0, x <= alpha), tf.float32)\n",
    "        alpha_grad_mask = tf.cast(x >= alpha, tf.float32)\n",
    "        alpha_grad = tf.reduce_sum(dy * alpha_grad_mask)\n",
    "        x_grad = dy * x_grad_mask\n",
    "        \n",
    "        return [x_grad, alpha_grad]\n",
    "\n",
    "    return output, grad_fn\n",
    "\n",
    "@tf.custom_gradient\n",
    "def AlphaQuantize(x, alpha, bits):\n",
    "    output = tf.round(x * ((2**bits - 1) / alpha)) * (alpha / (2**bits - 1))\n",
    "    \n",
    "    def grad_fn(dy):\n",
    "        return [dy, None, None]\n",
    "    \n",
    "    return output, grad_fn\n",
    "\n",
    "class PACT(tf.keras.layers.Layer):\n",
    "    def __init__(self, quantize=False, bits=2.):\n",
    "        super(PACT, self).__init__()      \n",
    "        self.quantize = quantize\n",
    "        self.bits = bits\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        self.alpha = self.add_variable(\n",
    "            'alpha', shape=[], \n",
    "            initializer=tf.keras.initializers.Constant([10.], dtype=tf.float32),\n",
    "            regularizer=tf.keras.regularizers.l2(0.01))\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        outputs = AlphaClip(inputs, self.alpha)\n",
    "        if self.quantize:\n",
    "            with tf.name_scope('QA'):\n",
    "                outputs = AlphaQuantize(outputs, self.alpha, self.bits)\n",
    "                tf.summary.histogram('quantized_activation', outputs)\n",
    "        return outputs\n",
    "    \n",
    "    def get_config(self):\n",
    "        return {'quantize': self.quantize, 'bits': self.bits}\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.custom_gradient\n",
    "def SAWBQuantize(x, alpha, bits):\n",
    "    # Clip between -alpha and alpha\n",
    "    clipped = tf.clip_by_value(x, -alpha, alpha)\n",
    "    # Rescale to [0, alpha]\n",
    "    scaled = (clipped + alpha) / 2.\n",
    "    # Quantize.\n",
    "    quantized = tf.round(scaled * ((2**bits - 1) / alpha)) * (alpha / (2**bits - 1))\n",
    "    # Rescale to negative range.\n",
    "    output = (2 * quantized) - alpha\n",
    "    \n",
    "    def grad_fn(dy):\n",
    "        x_mask = tf.cast(tf.abs(x) <= alpha, tf.float32)\n",
    "        x_grad = dy * x_mask\n",
    "        return [x_grad, None, None]\n",
    "    return output, grad_fn\n",
    "\n",
    "class SAWBConv2D(tf.keras.layers.Conv2D):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(SAWBConv2D, self).__init__(*args, **kwargs)\n",
    "        # For now hardcode coefficients.\n",
    "        self.c1 = 3.2\n",
    "        self.c2 = -2.1\n",
    "        self.bits = 2.\n",
    "        self.alpha = None\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        # Compute proper scale for our weights.\n",
    "        alpha = self.c1 * tf.sqrt(tf.reduce_mean(self.kernel**2)) + self.c2 * tf.reduce_mean(tf.abs(self.kernel))\n",
    "        self.alpha = alpha\n",
    "        # Quantize kernel\n",
    "        with tf.name_scope(\"QW\"):\n",
    "            q_kernel = SAWBQuantize(self.kernel, alpha, self.bits)\n",
    "            print(alpha)\n",
    "            print(q_kernel)\n",
    "            tf.summary.histogram(\"quantized_weight\", q_kernel)\n",
    "        \n",
    "        # Invoke convolution\n",
    "        outputs = self._convolution_op(inputs, q_kernel)\n",
    "        \n",
    "        if self.use_bias:\n",
    "            if self.data_format == 'channels_first':\n",
    "                outputs = tf.nn.bias_add(\n",
    "                    outputs, self.bias, data_format='NCHW')\n",
    "            else:\n",
    "                outputs = tf.nn.bias_add(\n",
    "                    outputs, self.bias, data_format='NHWC')\n",
    "\n",
    "        if self.activation is not None:\n",
    "            outputs = self.activation(outputs)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = {\n",
    "    'VGG11': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
    "    'VGG13': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
    "    'VGG16': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n",
    "    'VGG19': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'],\n",
    "}\n",
    "\n",
    "class VGG(tf.keras.models.Model):\n",
    "    def __init__(self, name, *args, **kwargs):\n",
    "        super(VGG, self).__init__(*args, **kwargs)\n",
    "        self.features = self._make_layers(cfg[name])\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.classifier = nn.Dense(10, activation=None)\n",
    "    \n",
    "    def call(self, inputs, training=True):\n",
    "        features = self.features(inputs, training=training)\n",
    "        features = self.flatten(features)\n",
    "        output = self.classifier(features)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def _make_layers(self, cfg):\n",
    "        layers = [nn.Conv2D(cfg[0], kernel_size=3, padding='same'), nn.BatchNormalization(), nn.Activation('relu')]\n",
    "        for x in cfg[1:]:\n",
    "            if x == 'M':\n",
    "                layers += [nn.MaxPool2D(pool_size=2, strides=2)]\n",
    "            else:\n",
    "                layers += [#nn.Conv2D(x, kernel_size=3, padding='same'),\n",
    "                          SAWBConv2D(x, kernel_size=3, padding='same'),\n",
    "                           nn.BatchNormalization(),\n",
    "                           #PACT(quantize=True)]\n",
    "                           nn.Activation('relu')]\n",
    "        layers += [nn.GlobalAveragePooling2D()]\n",
    "        \n",
    "        return tf.keras.models.Sequential(layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VGG('VGG11')\n",
    "test_input = tf.random_normal(shape=[1, 32, 32, 3])\n",
    "test_output = model(test_input, training=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/checkpointable/util.py:1455: NameBasedSaverStatus.__init__ (from tensorflow.python.training.checkpointable.util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Restoring a name-based tf.train.Saver checkpoint using the object-based restore API. This mode uses global names to match variables, and so is somewhat fragile. It also adds new restore ops to the graph each time it is called when graph building. Prefer re-encoding training checkpoints in the object-based format: run save() on the object-based saver (the same one this message is coming from) and use that checkpoint in the future.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.checkpointable.util.NameBasedSaverStatus at 0x7f6f998184a8>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_weights('/data/jwfromm/cifar_models/vgg_pact_a2/model.ckpt-23460')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=1511, shape=(1, 10), dtype=float32, numpy=\n",
       "array([[-6.6998057, -5.364214 , -0.924089 , -3.8044078, -0.9380314,\n",
       "        -6.679875 ,  4.0117774, -6.212219 , -5.8784456, -4.354133 ]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(test_input, training=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=1542, shape=(1, 32, 32, 64), dtype=float32, numpy=\n",
       "array([[[[0.        , 0.        , 0.        , ..., 0.67017335,\n",
       "          0.        , 0.        ],\n",
       "         [0.44678223, 0.        , 0.67017335, ..., 0.67017335,\n",
       "          0.        , 0.22339112],\n",
       "         [0.67017335, 0.        , 0.67017335, ..., 0.        ,\n",
       "          0.        , 0.        ],\n",
       "         ...,\n",
       "         [0.        , 0.67017335, 0.        , ..., 0.        ,\n",
       "          0.67017335, 0.        ],\n",
       "         [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "          0.67017335, 0.        ],\n",
       "         [0.        , 0.        , 0.        , ..., 0.67017335,\n",
       "          0.        , 0.        ]],\n",
       "\n",
       "        [[0.67017335, 0.        , 0.        , ..., 0.67017335,\n",
       "          0.67017335, 0.        ],\n",
       "         [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "          0.67017335, 0.22339112],\n",
       "         [0.        , 0.44678223, 0.        , ..., 0.        ,\n",
       "          0.67017335, 0.22339112],\n",
       "         ...,\n",
       "         [0.        , 0.67017335, 0.        , ..., 0.67017335,\n",
       "          0.        , 0.        ],\n",
       "         [0.67017335, 0.        , 0.67017335, ..., 0.        ,\n",
       "          0.        , 0.44678223],\n",
       "         [0.        , 0.        , 0.22339112, ..., 0.67017335,\n",
       "          0.        , 0.        ]],\n",
       "\n",
       "        [[0.        , 0.        , 0.67017335, ..., 0.22339112,\n",
       "          0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "          0.        , 0.22339112],\n",
       "         [0.        , 0.22339112, 0.67017335, ..., 0.        ,\n",
       "          0.        , 0.44678223],\n",
       "         ...,\n",
       "         [0.67017335, 0.        , 0.        , ..., 0.67017335,\n",
       "          0.        , 0.        ],\n",
       "         [0.67017335, 0.        , 0.        , ..., 0.        ,\n",
       "          0.44678223, 0.        ],\n",
       "         [0.67017335, 0.        , 0.67017335, ..., 0.        ,\n",
       "          0.67017335, 0.22339112]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.        , 0.        , 0.        , ..., 0.67017335,\n",
       "          0.22339112, 0.        ],\n",
       "         [0.        , 0.        , 0.        , ..., 0.67017335,\n",
       "          0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "          0.        , 0.        ],\n",
       "         ...,\n",
       "         [0.        , 0.67017335, 0.        , ..., 0.        ,\n",
       "          0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        , ..., 0.67017335,\n",
       "          0.        , 0.        ],\n",
       "         [0.22339112, 0.        , 0.67017335, ..., 0.        ,\n",
       "          0.        , 0.22339112]],\n",
       "\n",
       "        [[0.        , 0.67017335, 0.67017335, ..., 0.67017335,\n",
       "          0.22339112, 0.        ],\n",
       "         [0.        , 0.22339112, 0.        , ..., 0.        ,\n",
       "          0.        , 0.22339112],\n",
       "         [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "          0.        , 0.        ],\n",
       "         ...,\n",
       "         [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "          0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        , ..., 0.67017335,\n",
       "          0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "          0.        , 0.        ]],\n",
       "\n",
       "        [[0.        , 0.67017335, 0.        , ..., 0.        ,\n",
       "          0.        , 0.22339112],\n",
       "         [0.        , 0.        , 0.67017335, ..., 0.        ,\n",
       "          0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        , ..., 0.67017335,\n",
       "          0.22339112, 0.        ],\n",
       "         ...,\n",
       "         [0.        , 0.        , 0.67017335, ..., 0.        ,\n",
       "          0.        , 0.        ],\n",
       "         [0.67017335, 0.        , 0.67017335, ..., 0.        ,\n",
       "          0.44678223, 0.        ],\n",
       "         [0.        , 0.        , 0.        , ..., 0.67017335,\n",
       "          0.67017335, 0.        ]]]], dtype=float32)>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.keras.models.Sequential(model.layers[0].layers[:3])(test_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
