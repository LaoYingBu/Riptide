{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import tvm\n",
    "import tvm.relay as relay\n",
    "import tvm.contrib.graph_runtime as runtime\n",
    "from tvm.relay.expr_functor import ExprMutator\n",
    "import riptide.models\n",
    "from riptide.get_models import get_model\n",
    "from riptide.binary.binary_layers import Config, DQuantize, XQuantize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config(actQ=DQuantize, weightQ=XQuantize, bits=1, use_act=False, use_bn=False, use_maxpool=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with config:\n",
    "    model = get_model('vggnet')\n",
    "#model = riptide.models.vggnet_normal.vggnet()\n",
    "#model = tf.keras.models.Sequential(model.layers[:30])\n",
    "#model = tf.keras.models.Sequential()\n",
    "#model.add(tf.keras.layers.Conv2D(filters=96, strides=2, padding='same', kernel_size=7, use_bias=False, data_format='channels_last'))\n",
    "#model.add(tf.keras.layers.BatchNormalization(center=False, scale=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0715 22:44:29.646168 139757493847872 deprecation.py:323] From /home/Riptide/riptide/binary/binary_layers.py:526: _colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "W0715 22:44:29.681277 139757493847872 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/autograph/impl/api.py:279: calling Layer.add_update (from tensorflow.python.keras.engine.base_layer) with inputs is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "`inputs` is now automatically inferred\n"
     ]
    }
   ],
   "source": [
    "test_input = tf.keras.Input(shape=[224, 224, 3], batch_size=1, dtype='float32')\n",
    "output = model(test_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "func, params = relay.frontend.from_keras(model, shape={'input_1': [1, 224, 224, 3]}, layout='NHWC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v0.0.1\n",
      "def @main(%input_1: Tensor[(1, 224, 224, 3), float32], %v_param_1: Tensor[(3, 3, 96, 256), float32], %v_param_2: Tensor[(256,), float32], %v_param_3: Tensor[(256,), float32], %v_param_4: Tensor[(3, 3, 256, 256), float32], %v_param_5: Tensor[(256,), float32], %v_param_6: Tensor[(256,), float32], %v_param_7: Tensor[(3, 3, 256, 256), float32], %v_param_8: Tensor[(256,), float32], %v_param_9: Tensor[(256,), float32], %v_param_10: Tensor[(3, 3, 256, 512), float32], %v_param_11: Tensor[(512,), float32], %v_param_12: Tensor[(512,), float32], %v_param_13: Tensor[(3, 3, 512, 512), float32], %v_param_14: Tensor[(512,), float32], %v_param_15: Tensor[(512,), float32], %v_param_16: Tensor[(3, 3, 512, 512), float32], %v_param_17: Tensor[(512,), float32], %v_param_18: Tensor[(512,), float32], %v_param_19: Tensor[(3, 3, 512, 512), float32], %v_param_20: Tensor[(512,), float32], %v_param_21: Tensor[(512,), float32], %v_param_22: Tensor[(3, 3, 512, 512), float32], %v_param_23: Tensor[(512,), float32], %v_param_24: Tensor[(512,), float32], %v_param_25: Tensor[(3, 3, 512, 512), float32], %v_param_26: Tensor[(512,), float32], %v_param_27: Tensor[(512,), float32], %v_param_28: Tensor[(4096, 25088), float32], %v_param_29: Tensor[(4096,), float32], %v_param_30: Tensor[(4096,), float32], %v_param_31: Tensor[(4096, 4096), float32], %v_param_32: Tensor[(4096,), float32], %v_param_33: Tensor[(4096,), float32], %v_param_34: Tensor[(1000, 4096), float32], %v_param_35: Tensor[(1,), float32]) -> Tensor[(1, 1000), float32] {\n",
      "  %0 = nn.max_pool2d(%input_1, pool_size=[2, 2], strides=[2, 2], layout=\"NHWC\") /* ty=Tensor[(1, 112, 112, 3), float32] */\n",
      "  %1 = multiply(%0, 1f /* ty=float32 */) /* ty=Tensor[(1, 112, 112, 3), float32] */\n",
      "  %2 = clip(%1, a_min=0, a_max=1) /* ty=Tensor[(1, 112, 112, 3), float32] */\n",
      "  %3 = multiply(1f /* ty=float32 */, %2) /* ty=Tensor[(1, 112, 112, 3), float32] */\n",
      "  %4 = round(%3) /* ty=Tensor[(1, 112, 112, 3), float32] */\n",
      "  %5 = cast(%4, dtype=\"int16\") /* ty=Tensor[(1, 112, 112, 3), int16] */\n",
      "  %6 = cast(%v_param_1, dtype=\"int16\") /* ty=Tensor[(3, 3, 96, 256), int16] */\n",
      "  %7 = nn.bitpack(%6, pack_axis=2, bit_axis=2, pack_type=\"uint8\") /* ty=Tensor[(3, 3, 1, 12, 256), uint8] */\n",
      "  %8 = nn.bitserial_conv2d(%5, %7, padding=[1, 1], channels=256, data_layout=\"NHWC\", kernel_layout=\"\", pack_dtype=\"uint8\", out_dtype=\"int16\") /* ty=Tensor[(1, 112, 112, 256), int16] */\n",
      "  %9 = nn.relu(%8) /* ty=Tensor[(1, 112, 112, 256), int16] */\n",
      "  %10 = cast(%v_param_2, dtype=\"int16\") /* ty=Tensor[(256,), int16] */\n",
      "  %11 = add(%9, %10) /* ty=Tensor[(1, 112, 112, 256), int16] */\n",
      "  %12 = cast(%v_param_3, dtype=\"int16\") /* ty=Tensor[(256,), int16] */\n",
      "  %13 = right_shift(%11, %12) /* ty=Tensor[(1, 112, 112, 256), int16] */\n",
      "  %14 = cast(%v_param_4, dtype=\"int16\") /* ty=Tensor[(3, 3, 256, 256), int16] */\n",
      "  %15 = nn.bitpack(%14, pack_axis=2, bit_axis=2, pack_type=\"uint8\") /* ty=Tensor[(3, 3, 1, 32, 256), uint8] */\n",
      "  %16 = nn.bitserial_conv2d(%13, %15, padding=[1, 1], channels=256, data_layout=\"NHWC\", kernel_layout=\"\", pack_dtype=\"uint8\", out_dtype=\"int16\") /* ty=Tensor[(1, 112, 112, 256), int16] */\n",
      "  %17 = nn.relu(%16) /* ty=Tensor[(1, 112, 112, 256), int16] */\n",
      "  %18 = cast(%v_param_5, dtype=\"int16\") /* ty=Tensor[(256,), int16] */\n",
      "  %19 = add(%17, %18) /* ty=Tensor[(1, 112, 112, 256), int16] */\n",
      "  %20 = cast(%v_param_6, dtype=\"int16\") /* ty=Tensor[(256,), int16] */\n",
      "  %21 = right_shift(%19, %20) /* ty=Tensor[(1, 112, 112, 256), int16] */\n",
      "  %22 = cast(%v_param_7, dtype=\"int16\") /* ty=Tensor[(3, 3, 256, 256), int16] */\n",
      "  %23 = nn.bitpack(%22, pack_axis=2, bit_axis=2, pack_type=\"uint8\") /* ty=Tensor[(3, 3, 1, 32, 256), uint8] */\n",
      "  %24 = nn.bitserial_conv2d(%21, %23, padding=[1, 1], channels=256, data_layout=\"NHWC\", kernel_layout=\"\", pack_dtype=\"uint8\", out_dtype=\"int16\") /* ty=Tensor[(1, 112, 112, 256), int16] */\n",
      "  %25 = nn.relu(%24) /* ty=Tensor[(1, 112, 112, 256), int16] */\n",
      "  %26 = nn.max_pool2d(%25, pool_size=[2, 2], strides=[2, 2], layout=\"NHWC\") /* ty=Tensor[(1, 56, 56, 256), int16] */\n",
      "  %27 = cast(%v_param_8, dtype=\"int16\") /* ty=Tensor[(256,), int16] */\n",
      "  %28 = add(%26, %27) /* ty=Tensor[(1, 56, 56, 256), int16] */\n",
      "  %29 = cast(%v_param_9, dtype=\"int16\") /* ty=Tensor[(256,), int16] */\n",
      "  %30 = right_shift(%28, %29) /* ty=Tensor[(1, 56, 56, 256), int16] */\n",
      "  %31 = cast(%v_param_10, dtype=\"int16\") /* ty=Tensor[(3, 3, 256, 512), int16] */\n",
      "  %32 = nn.bitpack(%31, pack_axis=2, bit_axis=2, pack_type=\"uint8\") /* ty=Tensor[(3, 3, 1, 32, 512), uint8] */\n",
      "  %33 = nn.bitserial_conv2d(%30, %32, padding=[1, 1], channels=512, data_layout=\"NHWC\", kernel_layout=\"\", pack_dtype=\"uint8\", out_dtype=\"int16\") /* ty=Tensor[(1, 56, 56, 512), int16] */\n",
      "  %34 = nn.relu(%33) /* ty=Tensor[(1, 56, 56, 512), int16] */\n",
      "  %35 = cast(%v_param_11, dtype=\"int16\") /* ty=Tensor[(512,), int16] */\n",
      "  %36 = add(%34, %35) /* ty=Tensor[(1, 56, 56, 512), int16] */\n",
      "  %37 = cast(%v_param_12, dtype=\"int16\") /* ty=Tensor[(512,), int16] */\n",
      "  %38 = right_shift(%36, %37) /* ty=Tensor[(1, 56, 56, 512), int16] */\n",
      "  %39 = cast(%v_param_13, dtype=\"int16\") /* ty=Tensor[(3, 3, 512, 512), int16] */\n",
      "  %40 = nn.bitpack(%39, pack_axis=2, bit_axis=2, pack_type=\"uint8\") /* ty=Tensor[(3, 3, 1, 64, 512), uint8] */\n",
      "  %41 = nn.bitserial_conv2d(%38, %40, padding=[1, 1], channels=512, data_layout=\"NHWC\", kernel_layout=\"\", pack_dtype=\"uint8\", out_dtype=\"int16\") /* ty=Tensor[(1, 56, 56, 512), int16] */\n",
      "  %42 = nn.relu(%41) /* ty=Tensor[(1, 56, 56, 512), int16] */\n",
      "  %43 = cast(%v_param_14, dtype=\"int16\") /* ty=Tensor[(512,), int16] */\n",
      "  %44 = add(%42, %43) /* ty=Tensor[(1, 56, 56, 512), int16] */\n",
      "  %45 = cast(%v_param_15, dtype=\"int16\") /* ty=Tensor[(512,), int16] */\n",
      "  %46 = right_shift(%44, %45) /* ty=Tensor[(1, 56, 56, 512), int16] */\n",
      "  %47 = cast(%v_param_16, dtype=\"int16\") /* ty=Tensor[(3, 3, 512, 512), int16] */\n",
      "  %48 = nn.bitpack(%47, pack_axis=2, bit_axis=2, pack_type=\"uint8\") /* ty=Tensor[(3, 3, 1, 64, 512), uint8] */\n",
      "  %49 = nn.bitserial_conv2d(%46, %48, padding=[1, 1], channels=512, data_layout=\"NHWC\", kernel_layout=\"\", pack_dtype=\"uint8\", out_dtype=\"int16\") /* ty=Tensor[(1, 56, 56, 512), int16] */\n",
      "  %50 = nn.relu(%49) /* ty=Tensor[(1, 56, 56, 512), int16] */\n",
      "  %51 = nn.max_pool2d(%50, pool_size=[2, 2], strides=[2, 2], layout=\"NHWC\") /* ty=Tensor[(1, 28, 28, 512), int16] */\n",
      "  %52 = cast(%v_param_17, dtype=\"int16\") /* ty=Tensor[(512,), int16] */\n",
      "  %53 = add(%51, %52) /* ty=Tensor[(1, 28, 28, 512), int16] */\n",
      "  %54 = cast(%v_param_18, dtype=\"int16\") /* ty=Tensor[(512,), int16] */\n",
      "  %55 = right_shift(%53, %54) /* ty=Tensor[(1, 28, 28, 512), int16] */\n",
      "  %56 = cast(%v_param_19, dtype=\"int16\") /* ty=Tensor[(3, 3, 512, 512), int16] */\n",
      "  %57 = nn.bitpack(%56, pack_axis=2, bit_axis=2, pack_type=\"uint8\") /* ty=Tensor[(3, 3, 1, 64, 512), uint8] */\n",
      "  %58 = nn.bitserial_conv2d(%55, %57, padding=[1, 1], channels=512, data_layout=\"NHWC\", kernel_layout=\"\", pack_dtype=\"uint8\", out_dtype=\"int16\") /* ty=Tensor[(1, 28, 28, 512), int16] */\n",
      "  %59 = nn.relu(%58) /* ty=Tensor[(1, 28, 28, 512), int16] */\n",
      "  %60 = cast(%v_param_20, dtype=\"int16\") /* ty=Tensor[(512,), int16] */\n",
      "  %61 = add(%59, %60) /* ty=Tensor[(1, 28, 28, 512), int16] */\n",
      "  %62 = cast(%v_param_21, dtype=\"int16\") /* ty=Tensor[(512,), int16] */\n",
      "  %63 = right_shift(%61, %62) /* ty=Tensor[(1, 28, 28, 512), int16] */\n",
      "  %64 = cast(%v_param_22, dtype=\"int16\") /* ty=Tensor[(3, 3, 512, 512), int16] */\n",
      "  %65 = nn.bitpack(%64, pack_axis=2, bit_axis=2, pack_type=\"uint8\") /* ty=Tensor[(3, 3, 1, 64, 512), uint8] */\n",
      "  %66 = nn.bitserial_conv2d(%63, %65, padding=[1, 1], channels=512, data_layout=\"NHWC\", kernel_layout=\"\", pack_dtype=\"uint8\", out_dtype=\"int16\") /* ty=Tensor[(1, 28, 28, 512), int16] */\n",
      "  %67 = nn.relu(%66) /* ty=Tensor[(1, 28, 28, 512), int16] */\n",
      "  %68 = cast(%v_param_23, dtype=\"int16\") /* ty=Tensor[(512,), int16] */\n",
      "  %69 = add(%67, %68) /* ty=Tensor[(1, 28, 28, 512), int16] */\n",
      "  %70 = cast(%v_param_24, dtype=\"int16\") /* ty=Tensor[(512,), int16] */\n",
      "  %71 = right_shift(%69, %70) /* ty=Tensor[(1, 28, 28, 512), int16] */\n",
      "  %72 = cast(%v_param_25, dtype=\"int16\") /* ty=Tensor[(3, 3, 512, 512), int16] */\n",
      "  %73 = nn.bitpack(%72, pack_axis=2, bit_axis=2, pack_type=\"uint8\") /* ty=Tensor[(3, 3, 1, 64, 512), uint8] */\n",
      "  %74 = nn.bitserial_conv2d(%71, %73, padding=[1, 1], channels=512, data_layout=\"NHWC\", kernel_layout=\"\", pack_dtype=\"uint8\", out_dtype=\"int16\") /* ty=Tensor[(1, 28, 28, 512), int16] */\n",
      "  %75 = nn.relu(%74) /* ty=Tensor[(1, 28, 28, 512), int16] */\n",
      "  %76 = nn.max_pool2d(%75, pool_size=[2, 2], strides=[2, 2], layout=\"NHWC\") /* ty=Tensor[(1, 14, 14, 512), int16] */\n",
      "  %77 = cast(%v_param_26, dtype=\"int16\") /* ty=Tensor[(512,), int16] */\n",
      "  %78 = add(%76, %77) /* ty=Tensor[(1, 14, 14, 512), int16] */\n",
      "  %79 = cast(%v_param_27, dtype=\"int16\") /* ty=Tensor[(512,), int16] */\n",
      "  %80 = right_shift(%78, %79) /* ty=Tensor[(1, 14, 14, 512), int16] */\n",
      "  %81 = nn.batch_flatten(%80) /* ty=Tensor[(1, 100352), int16] */\n",
      "  %82 = cast(%v_param_28, dtype=\"int16\") /* ty=Tensor[(4096, 25088), int16] */\n",
      "  %83 = nn.bitpack(%82, bit_axis=1, pack_type=\"uint8\") /* ty=Tensor[(4096, 1, 3136), uint8] */\n",
      "  %84 = nn.bitserial_dense(%81, %83, units=4096, pack_dtype=\"uint8\", out_dtype=\"int16\") /* ty=Tensor[(1, 4096), int16] */\n",
      "  %85 = nn.relu(%84) /* ty=Tensor[(1, 4096), int16] */\n",
      "  %86 = cast(%v_param_29, dtype=\"int16\") /* ty=Tensor[(4096,), int16] */\n",
      "  %87 = add(%85, %86) /* ty=Tensor[(1, 4096), int16] */\n",
      "  %88 = cast(%v_param_30, dtype=\"int16\") /* ty=Tensor[(4096,), int16] */\n",
      "  %89 = right_shift(%87, %88) /* ty=Tensor[(1, 4096), int16] */\n",
      "  %90 = cast(%v_param_31, dtype=\"int16\") /* ty=Tensor[(4096, 4096), int16] */\n",
      "  %91 = nn.bitpack(%90, bit_axis=1, pack_type=\"uint8\") /* ty=Tensor[(4096, 1, 512), uint8] */\n",
      "  %92 = nn.bitserial_dense(%89, %91, units=4096, pack_dtype=\"uint8\", out_dtype=\"int16\") /* ty=Tensor[(1, 4096), int16] */\n",
      "  %93 = nn.relu(%92) /* ty=Tensor[(1, 4096), int16] */\n",
      "  %94 = cast(%v_param_32, dtype=\"int16\") /* ty=Tensor[(4096,), int16] */\n",
      "  %95 = add(%93, %94) /* ty=Tensor[(1, 4096), int16] */\n",
      "  %96 = cast(%v_param_33, dtype=\"int16\") /* ty=Tensor[(4096,), int16] */\n",
      "  %97 = right_shift(%95, %96) /* ty=Tensor[(1, 4096), int16] */\n",
      "  %98 = cast(%v_param_34, dtype=\"int16\") /* ty=Tensor[(1000, 4096), int16] */\n",
      "  %99 = nn.bitpack(%98, bit_axis=1, pack_type=\"uint8\") /* ty=Tensor[(1000, 1, 512), uint8] */\n",
      "  %100 = nn.bitserial_dense(%97, %99, units=1000, pack_dtype=\"uint8\", out_dtype=\"int16\") /* ty=Tensor[(1, 1000), int16] */\n",
      "  %101 = cast(%100, dtype=\"float32\") /* ty=Tensor[(1, 1000), float32] */\n",
      "  %102 = multiply(%101, %v_param_35) /* ty=Tensor[(1, 1000), float32] */\n",
      "  nn.softmax(%102, axis=1) /* ty=Tensor[(1, 1000), float32] */\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0715 22:44:35.888565 139757493847872 dispatcher.py:381] Cannot find config for target=llvm, workload=('bitserial_dense', (1, 4096, 'int16'), (1000, 1, 512, 'uint8'), 1, 1, 'uint8', 'int16', 1). A fallback configuration is used, which may bring great performance regression.\n",
      "W0715 22:44:35.894528 139757493847872 dispatcher.py:381] Cannot find config for target=llvm, workload=('bitserial_dense', (1, 4096, 'int16'), (4096, 1, 512, 'uint8'), 1, 1, 'uint8', 'int16', 1). A fallback configuration is used, which may bring great performance regression.\n",
      "W0715 22:44:35.899775 139757493847872 dispatcher.py:381] Cannot find config for target=llvm, workload=('bitserial_dense', (1, 100352, 'int16'), (4096, 1, 3136, 'uint8'), 1, 1, 'uint8', 'int16', 1). A fallback configuration is used, which may bring great performance regression.\n",
      "W0715 22:44:35.907642 139757493847872 dispatcher.py:381] Cannot find config for target=llvm, workload=('bitserial_conv2d_nhwc', (1, 28, 28, 512, 'int16'), (3, 3, 1, 64, 512, 'uint8'), (1, 1), (1, 1), 1, 1, 'uint8', 'int16', 1). A fallback configuration is used, which may bring great performance regression.\n",
      "W0715 22:44:35.982795 139757493847872 dispatcher.py:381] Cannot find config for target=llvm, workload=('bitserial_conv2d_nhwc', (1, 56, 56, 512, 'int16'), (3, 3, 1, 64, 512, 'uint8'), (1, 1), (1, 1), 1, 1, 'uint8', 'int16', 1). A fallback configuration is used, which may bring great performance regression.\n",
      "W0715 22:44:36.055972 139757493847872 dispatcher.py:381] Cannot find config for target=llvm, workload=('bitserial_conv2d_nhwc', (1, 56, 56, 256, 'int16'), (3, 3, 1, 32, 512, 'uint8'), (1, 1), (1, 1), 1, 1, 'uint8', 'int16', 1). A fallback configuration is used, which may bring great performance regression.\n",
      "W0715 22:44:36.131099 139757493847872 dispatcher.py:381] Cannot find config for target=llvm, workload=('bitserial_conv2d_nhwc', (1, 112, 112, 256, 'int16'), (3, 3, 1, 32, 256, 'uint8'), (1, 1), (1, 1), 1, 1, 'uint8', 'int16', 1). A fallback configuration is used, which may bring great performance regression.\n",
      "W0715 22:44:36.167698 139757493847872 dispatcher.py:381] Cannot find config for target=llvm, workload=('bitserial_conv2d_nhwc', (1, 112, 112, 3, 'int16'), (3, 3, 1, 12, 256, 'uint8'), (1, 1), (1, 1), 1, 1, 'uint8', 'int16', 1). A fallback configuration is used, which may bring great performance regression.\n"
     ]
    },
    {
     "ename": "TVMError",
     "evalue": "Traceback (most recent call last):\n  [bt] (8) /home/tvm/build/libtvm.so(tvm::relay::backend::GraphRuntimeCodegen::VisitExpr_(tvm::relay::CallNode const*)+0x60a) [0x7f1b41e7532a]\n  [bt] (7) /home/tvm/build/libtvm.so(+0x74c48c) [0x7f1b41e5148c]\n  [bt] (6) /home/tvm/build/libtvm.so(tvm::relay::CompileEngineImpl::LowerInternal(tvm::relay::CCacheKey const&)+0x43b) [0x7f1b41e5d54b]\n  [bt] (5) /home/tvm/build/libtvm.so(tvm::relay::ScheduleGetter::Create(tvm::relay::Function const&)+0x6b2) [0x7f1b41e5b8a2]\n  [bt] (4) /home/tvm/build/libtvm.so(tvm::relay::ScheduleGetter::VisitExpr(tvm::relay::Expr const&)+0xae) [0x7f1b41e5c3fe]\n  [bt] (3) /home/tvm/build/libtvm.so(tvm::relay::ExprFunctor<tvm::Array<tvm::Tensor, void> (tvm::relay::Expr const&)>::VisitExpr(tvm::relay::Expr const&)+0xd2) [0x7f1b41e58f22]\n  [bt] (2) /home/tvm/build/libtvm.so(std::_Function_handler<tvm::Array<tvm::Tensor, void> (tvm::NodeRef const&, tvm::relay::ExprFunctor<tvm::Array<tvm::Tensor, void> (tvm::relay::Expr const&)>*), tvm::relay::ExprFunctor<tvm::Array<tvm::Tensor, void> (tvm::relay::Expr const&)>::InitVTable()::{lambda(tvm::NodeRef const&, tvm::relay::ExprFunctor<tvm::Array<tvm::Tensor, void> (tvm::relay::Expr const&)>*)#6}>::_M_invoke(std::_Any_data const&, tvm::NodeRef const&, tvm::relay::ExprFunctor<tvm::Array<tvm::Tensor, void> (tvm::relay::Expr const&)>*&&)+0x34) [0x7f1b41e52014]\n  [bt] (1) /home/tvm/build/libtvm.so(tvm::relay::ScheduleGetter::VisitExpr_(tvm::relay::CallNode const*)+0x650) [0x7f1b41e59c40]\n  [bt] (0) /home/tvm/build/libtvm.so(+0xc077fb) [0x7f1b4230c7fb]\n  File \"tvm/_ffi/_cython/./function.pxi\", line 56, in tvm._ffi._cy3.core.tvm_callback\n  File \"/home/tvm/python/tvm/relay/op/nn/_nn.py\", line 559, in compute_bitserial_conv2d\n    weight_bits, pack_dtype, out_dtype, unipolar)\n  File \"</usr/local/lib/python3.6/dist-packages/decorator.py:decorator-gen-170>\", line 2, in bitserial_conv2d_nhwc\n  File \"/home/tvm/python/tvm/target.py\", line 372, in dispatch_func\n    return dispatch_dict[k](*args, **kwargs)\n  File \"</usr/local/lib/python3.6/dist-packages/decorator.py:decorator-gen-173>\", line 2, in config_dispatcher\n  File \"/home/tvm/python/tvm/autotvm/task/dispatcher.py\", line 215, in dispatch_func\n    return dispatch_dict['direct'](cfg, *args, **kwargs)\n  File \"/home/tvm/python/tvm/autotvm/task/topi_integration.py\", line 367, in template_call\n    node = f(cfg, *args, **kwargs)\n  File \"/home/tvm/topi/python/topi/nn/bitserial_conv2d.py\", line 323, in spatial_pack_nhwc\n    data_q = bitpack(data, in_bits, pack_axis=3, bit_axis=4, pack_type=pack_dtype)\n  File \"/home/tvm/topi/python/topi/nn/bitserial_util.py\", line 42, in bitpack\n    assert get_const_int(ishape[pack_axis]) % data_width == 0, \"Not a multiple of word size\"\nAssertionError: Not a multiple of word size",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTVMError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-993928c6f13c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mrelay\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt_level\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrelay\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'llvm'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;31m#out = intrp.evaluate(func)(np.random.uniform(size=(1, 3, 28, 28)))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tvm/python/tvm/relay/build_module.py\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(mod, target, target_host, params)\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtophub_context\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0mbld_mod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBuildModule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m         \u001b[0mgraph_json\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbld_mod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_host\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_json\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tvm/python/tvm/relay/build_module.py\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self, func, target, target_host, params)\u001b[0m\n\u001b[1;32m    107\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0;31m# Build the function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_host\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m         \u001b[0;31m# Get artifacts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0mgraph_json\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tvm/python/tvm/_ffi/_cython/function.pxi\u001b[0m in \u001b[0;36mtvm._ffi._cy3.core.FunctionBase.__call__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/tvm/python/tvm/_ffi/_cython/function.pxi\u001b[0m in \u001b[0;36mtvm._ffi._cy3.core.FuncCall\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/tvm/python/tvm/_ffi/_cython/function.pxi\u001b[0m in \u001b[0;36mtvm._ffi._cy3.core.FuncCall3\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/tvm/python/tvm/_ffi/_cython/base.pxi\u001b[0m in \u001b[0;36mtvm._ffi._cy3.core.CALL\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mTVMError\u001b[0m: Traceback (most recent call last):\n  [bt] (8) /home/tvm/build/libtvm.so(tvm::relay::backend::GraphRuntimeCodegen::VisitExpr_(tvm::relay::CallNode const*)+0x60a) [0x7f1b41e7532a]\n  [bt] (7) /home/tvm/build/libtvm.so(+0x74c48c) [0x7f1b41e5148c]\n  [bt] (6) /home/tvm/build/libtvm.so(tvm::relay::CompileEngineImpl::LowerInternal(tvm::relay::CCacheKey const&)+0x43b) [0x7f1b41e5d54b]\n  [bt] (5) /home/tvm/build/libtvm.so(tvm::relay::ScheduleGetter::Create(tvm::relay::Function const&)+0x6b2) [0x7f1b41e5b8a2]\n  [bt] (4) /home/tvm/build/libtvm.so(tvm::relay::ScheduleGetter::VisitExpr(tvm::relay::Expr const&)+0xae) [0x7f1b41e5c3fe]\n  [bt] (3) /home/tvm/build/libtvm.so(tvm::relay::ExprFunctor<tvm::Array<tvm::Tensor, void> (tvm::relay::Expr const&)>::VisitExpr(tvm::relay::Expr const&)+0xd2) [0x7f1b41e58f22]\n  [bt] (2) /home/tvm/build/libtvm.so(std::_Function_handler<tvm::Array<tvm::Tensor, void> (tvm::NodeRef const&, tvm::relay::ExprFunctor<tvm::Array<tvm::Tensor, void> (tvm::relay::Expr const&)>*), tvm::relay::ExprFunctor<tvm::Array<tvm::Tensor, void> (tvm::relay::Expr const&)>::InitVTable()::{lambda(tvm::NodeRef const&, tvm::relay::ExprFunctor<tvm::Array<tvm::Tensor, void> (tvm::relay::Expr const&)>*)#6}>::_M_invoke(std::_Any_data const&, tvm::NodeRef const&, tvm::relay::ExprFunctor<tvm::Array<tvm::Tensor, void> (tvm::relay::Expr const&)>*&&)+0x34) [0x7f1b41e52014]\n  [bt] (1) /home/tvm/build/libtvm.so(tvm::relay::ScheduleGetter::VisitExpr_(tvm::relay::CallNode const*)+0x650) [0x7f1b41e59c40]\n  [bt] (0) /home/tvm/build/libtvm.so(+0xc077fb) [0x7f1b4230c7fb]\n  File \"tvm/_ffi/_cython/./function.pxi\", line 56, in tvm._ffi._cy3.core.tvm_callback\n  File \"/home/tvm/python/tvm/relay/op/nn/_nn.py\", line 559, in compute_bitserial_conv2d\n    weight_bits, pack_dtype, out_dtype, unipolar)\n  File \"</usr/local/lib/python3.6/dist-packages/decorator.py:decorator-gen-170>\", line 2, in bitserial_conv2d_nhwc\n  File \"/home/tvm/python/tvm/target.py\", line 372, in dispatch_func\n    return dispatch_dict[k](*args, **kwargs)\n  File \"</usr/local/lib/python3.6/dist-packages/decorator.py:decorator-gen-173>\", line 2, in config_dispatcher\n  File \"/home/tvm/python/tvm/autotvm/task/dispatcher.py\", line 215, in dispatch_func\n    return dispatch_dict['direct'](cfg, *args, **kwargs)\n  File \"/home/tvm/python/tvm/autotvm/task/topi_integration.py\", line 367, in template_call\n    node = f(cfg, *args, **kwargs)\n  File \"/home/tvm/topi/python/topi/nn/bitserial_conv2d.py\", line 323, in spatial_pack_nhwc\n    data_q = bitpack(data, in_bits, pack_axis=3, bit_axis=4, pack_type=pack_dtype)\n  File \"/home/tvm/topi/python/topi/nn/bitserial_util.py\", line 42, in bitpack\n    assert get_const_int(ishape[pack_axis]) % data_width == 0, \"Not a multiple of word size\"\nAssertionError: Not a multiple of word size"
     ]
    }
   ],
   "source": [
    "with relay.build_config(opt_level=0):\n",
    "    graph, lib, params = relay.build(func, target='llvm', params=params)\n",
    "    #out = intrp.evaluate(func)(np.random.uniform(size=(1, 3, 28, 28)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "module = runtime.create(graph, lib, tvm.context('llvm', 0))\n",
    "module.set_input(**params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "module.set_input('input_1', np.random.uniform(size=(1, 224, 224, 3)))\n",
    "module.run()\n",
    "print(module.get_output(0).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate\n",
    "ctx = tvm.cpu()\n",
    "print(\"Evaluate inference time cost...\")\n",
    "ftimer = module.module.time_evaluator(\"run\", ctx, number=1, repeat=10)\n",
    "prof_res = np.array(ftimer().results) * 1000  # convert to millisecond\n",
    "print(\"Mean inference time (std dev): %.2f ms (%.2f ms)\" %\n",
    "      (np.mean(prof_res), np.std(prof_res)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_layer = tf.keras.layers.Conv2D(filters=10, kernel_size=3, data_format='channels_last')\n",
    "test_input = tf.keras.Input(batch_size=1, shape=[32, 32, 32])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_output = test_layer(test_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_layer.get_weights()[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_layer.input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
