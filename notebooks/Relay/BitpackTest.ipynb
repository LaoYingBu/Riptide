{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tvm\n",
    "import numpy as np\n",
    "import topi\n",
    "from tvm import relay\n",
    "import topi.testing\n",
    "from tvm.contrib import graph_runtime\n",
    "from topi.util import get_const_tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolution Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cannot find config for target=llvm, workload=('bitserial_conv2d_nchw', (1, 32, 32, 32, 'uint32'), (1, 32, 1, 3, 3, 'uint32'), (1, 1), (1, 1), 1, 1, 'uint32', 'int32', True). A fallback configuration is used, which may bring great performance regression.\n"
     ]
    }
   ],
   "source": [
    "batch = 1\n",
    "in_height = in_width = in_size = 32\n",
    "in_dim = 32\n",
    "out_dim = 32\n",
    "in_channel = 32\n",
    "num_filter = 32\n",
    "kernel = 3\n",
    "stride = (1, 1)\n",
    "padding = (1, 1)\n",
    "activation_bits = 1\n",
    "weight_bits = 1\n",
    "unipolar = True\n",
    "\n",
    "input_dtype = 'uint32'\n",
    "out_dtype = 'int32'\n",
    "\n",
    "def generate_quantized_np(shape, bits, out_dtype):\n",
    "    min_val = 0 \n",
    "    max_val = 1 << bits\n",
    "    return np.random.randint(min_val, max_val, size=shape).astype(out_dtype)\n",
    "\n",
    "with tvm.target.create('llvm'):\n",
    "    A = tvm.placeholder((batch, in_channel, in_height, in_width), dtype=input_dtype, name='A')\n",
    "    W = tvm.placeholder((num_filter, in_channel, kernel, kernel), dtype=input_dtype, name='W')\n",
    "    QW = topi.nn.bitpack(W, weight_bits, pack_axis=1, bit_axis=0)\n",
    "    B = topi.nn.bitserial_conv2d_nchw(A, QW, stride, padding, activation_bits, weight_bits,\n",
    "                                      out_dtype=out_dtype, unipolar=unipolar)\n",
    "    s = topi.generic.schedule_bitserial_conv2d_nchw([B])\n",
    "    \n",
    "a_shape = get_const_tuple(A.shape)\n",
    "w_shape = get_const_tuple(W.shape)\n",
    "\n",
    "a_np = generate_quantized_np(a_shape, activation_bits, input_dtype)\n",
    "w_np = generate_quantized_np(w_shape, weight_bits, input_dtype)\n",
    "\n",
    "if unipolar:\n",
    "    w_ = np.copy(w_np).astype(out_dtype)\n",
    "    for x in np.nditer(w_, op_flags=['readwrite']):\n",
    "        x[...] = 1 if x == 1 else -1\n",
    "    b_np = topi.testing.conv2d_nchw_python(a_np.astype(out_dtype), w_, stride, padding)\n",
    "else:\n",
    "    b_np = topi.testing.conv2d_nchw_python(a_np, w_np, stride, padding)\n",
    "    \n",
    "ctx = tvm.cpu(0)\n",
    "a = tvm.nd.array(a_np, ctx)\n",
    "w = tvm.nd.array(w_np, ctx)\n",
    "#b = tvm.nd.array(np.zeros(get_const_tuple(B.shape), dtype=B.dtype), ctx)\n",
    "b = tvm.nd.empty(get_const_tuple(B.shape), dtype=B.dtype, ctx=ctx)\n",
    "func = tvm.build(s, [A, W, B], 'llvm')\n",
    "func(a, w, b)\n",
    "tvm.testing.assert_allclose(b.asnumpy(), b_np, rtol=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_var = relay.var('input', shape=A.shape, dtype=A.dtype)\n",
    "kernel_var = relay.var('kernel', shape=W.shape, dtype=W.dtype)\n",
    "q_kernel = relay.nn.bitpack(kernel_var, bits=1, pack_axis=1, bit_axis=0)\n",
    "q_out = relay.nn.bitserial_conv2d(input_var, q_kernel, channels=32, kernel_size=(3,3), padding=(1, 1), data_layout='NCHW', pack_dtype='uint32', out_dtype='int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_func = relay.Function([input_var, kernel_var], q_out)\n",
    "\n",
    "with relay.build_config(opt_level=3):\n",
    "    graph, lib, params = relay.build(q_func, 'llvm', params={'kernel': w_np})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "module = graph_runtime.create(graph, lib, tvm.cpu())\n",
    "module.set_input('input', a_np)\n",
    "module.set_input(**params)\n",
    "module.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tvm.testing.assert_allclose(module.get_output(0).asnumpy(), b_np, rtol=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dense Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:autotvm:Cannot find config for target=llvm, workload=('bitserial_dense', (1, 32, 'uint32'), (32, 1, 1, 'uint32'), 1, 1, 'uint32', 'int32', True). A fallback configuration is used, which may bring great performance regression.\n"
     ]
    }
   ],
   "source": [
    "with tvm.target.create('llvm'):\n",
    "    A = tvm.placeholder((batch, in_dim), dtype=input_dtype, name='A')\n",
    "    B = tvm.placeholder((out_dim, in_dim), dtype=input_dtype, name='B')\n",
    "    QB = topi.nn.bitpack(B, bits=1, bit_axis=1, pack_axis=1)\n",
    "    C = topi.nn.bitserial_dense(A, QB, activation_bits, weight_bits, out_dtype=out_dtype,\n",
    "                                unipolar=unipolar)\n",
    "    s = topi.generic.schedule_bitserial_dense([C])\n",
    "\n",
    "a_shape = get_const_tuple(A.shape)\n",
    "b_shape = get_const_tuple(B.shape)\n",
    "\n",
    "a_np = generate_quantized_np(get_const_tuple(a_shape), activation_bits, input_dtype)\n",
    "b_np = generate_quantized_np(get_const_tuple(b_shape), weight_bits, input_dtype)\n",
    "if unipolar:\n",
    "    b_ = np.copy(b_np).astype(out_dtype)\n",
    "    for x in np.nditer(b_, op_flags=['readwrite']):\n",
    "        x[...] = 1 if x == 1 else -1\n",
    "    c_np = np.dot(a_np, b_.T)\n",
    "else:\n",
    "    c_np = np.dot(a_np, b_np.T)\n",
    "    \n",
    "a = tvm.nd.array(a_np, ctx)\n",
    "b = tvm.nd.array(b_np, ctx)\n",
    "c = tvm.nd.array(np.zeros(get_const_tuple(C.shape), dtype=C.dtype), ctx)\n",
    "func = tvm.build(s, [A, B, C], \"llvm\")\n",
    "func(a, b, c)\n",
    "tvm.testing.assert_allclose(c.asnumpy(), c_np, rtol=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_var = relay.var('input', shape=A.shape, dtype=A.dtype)\n",
    "kernel_var = relay.var('kernel', shape=B.shape, dtype=B.dtype)\n",
    "q_kernel = relay.nn.bitpack(kernel_var, bits=1, pack_axis=1, bit_axis=1)\n",
    "q_out = relay.nn.bitserial_dense(input_var, q_kernel, units=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:autotvm:Cannot find config for target=llvm, workload=('bitserial_dense', (1, 32, 'uint32'), (32, 1, 1, 'uint32'), 1, 1, 'uint32', 'int16', 1). A fallback configuration is used, which may bring great performance regression.\n"
     ]
    }
   ],
   "source": [
    "q_func = relay.Function([input_var, kernel_var], q_out)\n",
    "\n",
    "with relay.build_config(opt_level=3):\n",
    "    graph, lib, params = relay.build(q_func, 'llvm', params={'kernel': b_np})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "module = graph_runtime.create(graph, lib, tvm.cpu())\n",
    "module.set_input('input', a_np)\n",
    "module.set_input(**params)\n",
    "module.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tvm.testing.assert_allclose(module.get_output(0).asnumpy(), c_np, rtol=1e-5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
