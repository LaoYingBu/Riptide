{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tvm\n",
    "from tvm import relay\n",
    "from tvm import autotvm\n",
    "import tvm.contrib.graph_runtime as runtime\n",
    "from tvm.contrib import util\n",
    "from tvm.contrib.util import tempdir\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = tvm.target.arm_cpu(\"rasp3b\")\n",
    "target_host = 'llvm -device=arm_cpu -target=arm-linux-gnueabihf -mattr=+neon'\n",
    "\n",
    "#target = 'llvm'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = relay.var('x', shape=[1, 4096], dtype='int16')\n",
    "#w = relay.var('w', shape=[32, 32, 3, 3], dtype='int16')\n",
    "#w_np = np.random.normal(size=[32, 32, 3, 3]).astype('float32').astype('int16')\n",
    "w = relay.var('w', shape=[4096, 4096], dtype='int16')\n",
    "#w = relay.var('w', dtype='int16')\n",
    "w_np = np.random.normal(size=[4096, 4096]).astype('int16')\n",
    "x_np = np.random.normal(size=[1, 4096]).astype('int16')\n",
    "\n",
    "#y = relay.nn.conv2d(x, w, data_layout='NHWC', kernel_size=[3, 3], kernel_layout='OIHW')\n",
    "bpw = relay.nn.bitpack(w, bit_axis=1, pack_axis=1, bits=1, pack_type='uint8')\n",
    "#bpw = relay.nn.bitpack(w, bits=1, pack_axis=2, bit_axis=4, pack_type='uint8')\n",
    "y = relay.nn.bitserial_dense(x, bpw, units=4096, data_bits=1, weight_bits=1, unipolar=False, pack_dtype='uint8')\n",
    "y_func = relay.Function([x, w], y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v0.0.3\n",
      "free_var %x: Tensor[(1, 4096), int16]\n",
      "free_var %w: Tensor[(4096, 4096), int16]\n",
      "%0 = nn.bitpack(%w, bit_axis=1, pack_type=\"int8\");\n",
      "nn.bitserial_dense(%x, %0, units=4096, pack_dtype=\"int8\", out_dtype=\"int16\", unipolar=False)\n"
     ]
    }
   ],
   "source": [
    "print(y_func.body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cannot find config for target=llvm -device=arm_cpu -model=bcm2837 -target=armv7l-linux-gnueabihf -mattr=+neon, workload=('bitserial_dense', (1, 4096, 'int16'), (4096, 1, 512, 'int8'), 1, 1, 'int8', 'int16', 0). A fallback configuration is used, which may bring great performance regression.\n"
     ]
    },
    {
     "ename": "TVMError",
     "evalue": "Traceback (most recent call last):\n  [bt] (8) /home/tvm/build/libtvm.so(tvm::relay::backend::RelayBuildModule::GetFunction(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::shared_ptr<tvm::runtime::ModuleNode> const&)::{lambda(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)#3}::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const+0x96a) [0x7f474b8d25fa]\n  [bt] (7) /home/tvm/build/libtvm.so(tvm::relay::backend::RelayBuildModule::BuildRelay(tvm::relay::Function, std::unordered_map<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tvm::runtime::NDArray, std::hash<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::equal_to<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const, tvm::runtime::NDArray> > > const&)+0x7c4) [0x7f474b8d12f4]\n  [bt] (6) /home/tvm/build/libtvm.so(std::_Function_handler<void (tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*), tvm::relay::backend::GraphRuntimeCodegenModule::GetFunction(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::shared_ptr<tvm::runtime::ModuleNode> const&)::{lambda(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)#2}>::_M_invoke(std::_Any_data const&, tvm::runtime::TVMArgs&&, tvm::runtime::TVMRetValue*&&)+0x308) [0x7f474b8fd638]\n  [bt] (5) /home/tvm/build/libtvm.so(tvm::relay::backend::GraphRuntimeCodegen::Codegen(tvm::relay::Function)+0x536) [0x7f474b8fc6c6]\n  [bt] (4) /home/tvm/build/libtvm.so(tvm::relay::backend::GraphRuntimeCodegen::VisitExpr(tvm::relay::Expr const&)+0x703) [0x7f474b8f3f03]\n  [bt] (3) /home/tvm/build/libtvm.so(tvm::relay::backend::GraphRuntimeCodegen::VisitExpr_(tvm::relay::CallNode const*)+0x5fa) [0x7f474b8f761a]\n  [bt] (2) /home/tvm/build/libtvm.so(+0x9de6fc) [0x7f474b8d36fc]\n  [bt] (1) /home/tvm/build/libtvm.so(tvm::relay::CompileEngineImpl::LowerInternal(tvm::relay::CCacheKey const&)+0xeae) [0x7f474b8dfefe]\n  [bt] (0) /home/tvm/build/libtvm.so(+0xb61a4b) [0x7f474ba56a4b]\n  File \"/home/tvm/python/tvm/relay/backend/_backend.py\", line 51, in lower\n    f = _build.lower(sch, inputs, name=func_name)\n  File \"/home/tvm/python/tvm/build_module.py\", line 376, in lower\n    stmt = form_body(sch)\n  File \"/home/tvm/python/tvm/build_module.py\", line 326, in form_body\n    stmt = schedule.ScheduleOps(sch, bounds)\n  File \"tvm/_ffi/_cython/./function.pxi\", line 310, in tvm._ffi._cy3.core.FunctionBase.__call__\n  File \"tvm/_ffi/_cython/./function.pxi\", line 245, in tvm._ffi._cy3.core.FuncCall\n  File \"tvm/_ffi/_cython/./function.pxi\", line 234, in tvm._ffi._cy3.core.FuncCall3\n  File \"tvm/_ffi/_cython/./base.pxi\", line 171, in tvm._ffi._cy3.core.CALL\n  [bt] (7) /home/tvm/build/libtvm.so(TVMFuncCall+0x65) [0x7f474ba5ba25]\n  [bt] (6) /home/tvm/build/libtvm.so(+0x4301e9) [0x7f474b3251e9]\n  [bt] (5) /home/tvm/build/libtvm.so(tvm::schedule::ScheduleOps(tvm::Schedule, tvm::Map<tvm::IterVar, tvm::Range, void, void>, bool)+0x1354) [0x7f474b6670d4]\n  [bt] (4) /home/tvm/build/libtvm.so(tvm::schedule::MakePipeline(tvm::Stage const&, std::unordered_map<tvm::IterVar, tvm::Range, std::hash<tvm::IterVar>, std::equal_to<tvm::IterVar>, std::allocator<std::pair<tvm::IterVar const, tvm::Range> > > const&, tvm::Stmt, bool)+0x66) [0x7f474b665366]\n  [bt] (3) /home/tvm/build/libtvm.so(tvm::ComputeOpNode::BuildProvide(tvm::Stage const&, std::unordered_map<tvm::IterVar, tvm::Range, std::hash<tvm::IterVar>, std::equal_to<tvm::IterVar>, std::allocator<std::pair<tvm::IterVar const, tvm::Range> > > const&, bool) const+0x185) [0x7f474b4ce075]\n  [bt] (2) /home/tvm/build/libtvm.so(tvm::MakeTensorize(tvm::ComputeOpNode const*, tvm::Stage const&, std::unordered_map<tvm::IterVar, tvm::Range, std::hash<tvm::IterVar>, std::equal_to<tvm::IterVar>, std::allocator<std::pair<tvm::IterVar const, tvm::Range> > > const&, bool)+0x26d) [0x7f474b503c6d]\n  [bt] (1) /home/tvm/build/libtvm.so(tvm::VerifyTensorizeBody(tvm::ComputeOpNode const*, tvm::Stage const&, std::unordered_map<tvm::IterVar, tvm::Range, std::hash<tvm::IterVar>, std::equal_to<tvm::IterVar>, std::allocator<std::pair<tvm::IterVar const, tvm::Range> > > const&, std::unordered_map<tvm::Tensor, tvm::Array<tvm::Range, void>, std::hash<tvm::Tensor>, std::equal_to<tvm::Tensor>, std::allocator<std::pair<tvm::Tensor const, tvm::Array<tvm::Range, void> > > > const&, tvm::TensorIntrin const&)+0x73c) [0x7f474b50163c]\n  [bt] (0) /home/tvm/build/libtvm.so(dmlc::LogMessageFatal::~LogMessageFatal()+0x43) [0x7f474b2e19c3]\n  File \"/home/jwfromm/tvm/src/op/tensorize.cc\", line 333\n  File \"tvm/_ffi/_cython/./function.pxi\", line 56, in tvm._ffi._cy3.core.tvm_callback\n  File \"/home/tvm/python/tvm/relay/backend/_backend.py\", line 59, in lower\n    raise RuntimeError(msg)\n  File \"/home/tvm/python/tvm/relay/backend/_backend.py\", line 51, in lower\n    f = _build.lower(sch, inputs, name=func_name)\n  File \"/home/tvm/python/tvm/build_module.py\", line 376, in lower\n    stmt = form_body(sch)\n  File \"/home/tvm/python/tvm/build_module.py\", line 326, in form_body\n    stmt = schedule.ScheduleOps(sch, bounds)\n  File \"tvm/_ffi/_cython/./function.pxi\", line 310, in tvm._ffi._cy3.core.FunctionBase.__call__\n  File \"tvm/_ffi/_cython/./function.pxi\", line 245, in tvm._ffi._cy3.core.FuncCall\n  File \"tvm/_ffi/_cython/./function.pxi\", line 234, in tvm._ffi._cy3.core.FuncCall3\n  File \"tvm/_ffi/_cython/./base.pxi\", line 171, in tvm._ffi._cy3.core.CALL\n  [bt] (7) /home/tvm/build/libtvm.so(TVMFuncCall+0x65) [0x7f474ba5ba25]\n  [bt] (6) /home/tvm/build/libtvm.so(+0x4301e9) [0x7f474b3251e9]\n  [bt] (5) /home/tvm/build/libtvm.so(tvm::schedule::ScheduleOps(tvm::Schedule, tvm::Map<tvm::IterVar, tvm::Range, void, void>, bool)+0x1354) [0x7f474b6670d4]\n  [bt] (4) /home/tvm/build/libtvm.so(tvm::schedule::MakePipeline(tvm::Stage const&, std::unordered_map<tvm::IterVar, tvm::Range, std::hash<tvm::IterVar>, std::equal_to<tvm::IterVar>, std::allocator<std::pair<tvm::IterVar const, tvm::Range> > > const&, tvm::Stmt, bool)+0x66) [0x7f474b665366]\n  [bt] (3) /home/tvm/build/libtvm.so(tvm::ComputeOpNode::BuildProvide(tvm::Stage const&, std::unordered_map<tvm::IterVar, tvm::Range, std::hash<tvm::IterVar>, std::equal_to<tvm::IterVar>, std::allocator<std::pair<tvm::IterVar const, tvm::Range> > > const&, bool) const+0x185) [0x7f474b4ce075]\n  [bt] (2) /home/tvm/build/libtvm.so(tvm::MakeTensorize(tvm::ComputeOpNode const*, tvm::Stage const&, std::unordered_map<tvm::IterVar, tvm::Range, std::hash<tvm::IterVar>, std::equal_to<tvm::IterVar>, std::allocator<std::pair<tvm::IterVar const, tvm::Range> > > const&, bool)+0x26d) [0x7f474b503c6d]\n  [bt] (1) /home/tvm/build/libtvm.so(tvm::VerifyTensorizeBody(tvm::ComputeOpNode const*, tvm::Stage const&, std::unordered_map<tvm::IterVar, tvm::Range, std::hash<tvm::IterVar>, std::equal_to<tvm::IterVar>, std::allocator<std::pair<tvm::IterVar const, tvm::Range> > > const&, std::unordered_map<tvm::Tensor, tvm::Array<tvm::Range, void>, std::hash<tvm::Tensor>, std::equal_to<tvm::Tensor>, std::allocator<std::pair<tvm::Tensor const, tvm::Array<tvm::Range, void> > > > const&, tvm::TensorIntrin const&)+0x73c) [0x7f474b50163c]\n  [bt] (0) /home/tvm/build/libtvm.so(dmlc::LogMessageFatal::~LogMessageFatal()+0x43) [0x7f474b2e19c3]\n  File \"/home/jwfromm/tvm/src/op/tensorize.cc\", line 333\nTVMError: Failed to match the data type with TensorIntrin tensor_intrin's declaration  provided=int16, intrin=uint16\nDuring handling of the above exception, another exception occurred:\n\nTVMError: Failed to match the data type with TensorIntrin tensor_intrin's declaration  provided=int16, intrin=uint16\nError during compile function\n-----------------------------\nv0.0.3\nfn (%p0: Tensor[(1, 4096), int16], %p1: Tensor[(4096, 1, 512), int8], Primitive=1) -> Tensor[(1, 4096), int16] {\n  nn.bitserial_dense(%p0, %p1, units=4096, pack_dtype=\"int8\", out_dtype=\"int16\", unipolar=False) /* ty=Tensor[(1, 4096), int16] */\n}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTVMError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-66da6f935102>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'w'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mw_np\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mrelay\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt_level\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrelay\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/tvm/python/tvm/relay/build_module.py\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(mod, target, target_host, params)\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtophub_context\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m         \u001b[0mbld_mod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBuildModule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 207\u001b[0;31m         \u001b[0mgraph_json\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbld_mod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_host\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    208\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_json\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tvm/python/tvm/relay/build_module.py\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self, func, target, target_host, params)\u001b[0m\n\u001b[1;32m    106\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0;31m# Build the function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_host\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m         \u001b[0;31m# Get artifacts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0mgraph_json\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tvm/python/tvm/_ffi/_cython/function.pxi\u001b[0m in \u001b[0;36mtvm._ffi._cy3.core.FunctionBase.__call__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/tvm/python/tvm/_ffi/_cython/function.pxi\u001b[0m in \u001b[0;36mtvm._ffi._cy3.core.FuncCall\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/tvm/python/tvm/_ffi/_cython/function.pxi\u001b[0m in \u001b[0;36mtvm._ffi._cy3.core.FuncCall3\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/tvm/python/tvm/_ffi/_cython/base.pxi\u001b[0m in \u001b[0;36mtvm._ffi._cy3.core.CALL\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mTVMError\u001b[0m: Traceback (most recent call last):\n  [bt] (8) /home/tvm/build/libtvm.so(tvm::relay::backend::RelayBuildModule::GetFunction(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::shared_ptr<tvm::runtime::ModuleNode> const&)::{lambda(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)#3}::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const+0x96a) [0x7f474b8d25fa]\n  [bt] (7) /home/tvm/build/libtvm.so(tvm::relay::backend::RelayBuildModule::BuildRelay(tvm::relay::Function, std::unordered_map<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tvm::runtime::NDArray, std::hash<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::equal_to<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const, tvm::runtime::NDArray> > > const&)+0x7c4) [0x7f474b8d12f4]\n  [bt] (6) /home/tvm/build/libtvm.so(std::_Function_handler<void (tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*), tvm::relay::backend::GraphRuntimeCodegenModule::GetFunction(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::shared_ptr<tvm::runtime::ModuleNode> const&)::{lambda(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)#2}>::_M_invoke(std::_Any_data const&, tvm::runtime::TVMArgs&&, tvm::runtime::TVMRetValue*&&)+0x308) [0x7f474b8fd638]\n  [bt] (5) /home/tvm/build/libtvm.so(tvm::relay::backend::GraphRuntimeCodegen::Codegen(tvm::relay::Function)+0x536) [0x7f474b8fc6c6]\n  [bt] (4) /home/tvm/build/libtvm.so(tvm::relay::backend::GraphRuntimeCodegen::VisitExpr(tvm::relay::Expr const&)+0x703) [0x7f474b8f3f03]\n  [bt] (3) /home/tvm/build/libtvm.so(tvm::relay::backend::GraphRuntimeCodegen::VisitExpr_(tvm::relay::CallNode const*)+0x5fa) [0x7f474b8f761a]\n  [bt] (2) /home/tvm/build/libtvm.so(+0x9de6fc) [0x7f474b8d36fc]\n  [bt] (1) /home/tvm/build/libtvm.so(tvm::relay::CompileEngineImpl::LowerInternal(tvm::relay::CCacheKey const&)+0xeae) [0x7f474b8dfefe]\n  [bt] (0) /home/tvm/build/libtvm.so(+0xb61a4b) [0x7f474ba56a4b]\n  File \"/home/tvm/python/tvm/relay/backend/_backend.py\", line 51, in lower\n    f = _build.lower(sch, inputs, name=func_name)\n  File \"/home/tvm/python/tvm/build_module.py\", line 376, in lower\n    stmt = form_body(sch)\n  File \"/home/tvm/python/tvm/build_module.py\", line 326, in form_body\n    stmt = schedule.ScheduleOps(sch, bounds)\n  File \"tvm/_ffi/_cython/./function.pxi\", line 310, in tvm._ffi._cy3.core.FunctionBase.__call__\n  File \"tvm/_ffi/_cython/./function.pxi\", line 245, in tvm._ffi._cy3.core.FuncCall\n  File \"tvm/_ffi/_cython/./function.pxi\", line 234, in tvm._ffi._cy3.core.FuncCall3\n  File \"tvm/_ffi/_cython/./base.pxi\", line 171, in tvm._ffi._cy3.core.CALL\n  [bt] (7) /home/tvm/build/libtvm.so(TVMFuncCall+0x65) [0x7f474ba5ba25]\n  [bt] (6) /home/tvm/build/libtvm.so(+0x4301e9) [0x7f474b3251e9]\n  [bt] (5) /home/tvm/build/libtvm.so(tvm::schedule::ScheduleOps(tvm::Schedule, tvm::Map<tvm::IterVar, tvm::Range, void, void>, bool)+0x1354) [0x7f474b6670d4]\n  [bt] (4) /home/tvm/build/libtvm.so(tvm::schedule::MakePipeline(tvm::Stage const&, std::unordered_map<tvm::IterVar, tvm::Range, std::hash<tvm::IterVar>, std::equal_to<tvm::IterVar>, std::allocator<std::pair<tvm::IterVar const, tvm::Range> > > const&, tvm::Stmt, bool)+0x66) [0x7f474b665366]\n  [bt] (3) /home/tvm/build/libtvm.so(tvm::ComputeOpNode::BuildProvide(tvm::Stage const&, std::unordered_map<tvm::IterVar, tvm::Range, std::hash<tvm::IterVar>, std::equal_to<tvm::IterVar>, std::allocator<std::pair<tvm::IterVar const, tvm::Range> > > const&, bool) const+0x185) [0x7f474b4ce075]\n  [bt] (2) /home/tvm/build/libtvm.so(tvm::MakeTensorize(tvm::ComputeOpNode const*, tvm::Stage const&, std::unordered_map<tvm::IterVar, tvm::Range, std::hash<tvm::IterVar>, std::equal_to<tvm::IterVar>, std::allocator<std::pair<tvm::IterVar const, tvm::Range> > > const&, bool)+0x26d) [0x7f474b503c6d]\n  [bt] (1) /home/tvm/build/libtvm.so(tvm::VerifyTensorizeBody(tvm::ComputeOpNode const*, tvm::Stage const&, std::unordered_map<tvm::IterVar, tvm::Range, std::hash<tvm::IterVar>, std::equal_to<tvm::IterVar>, std::allocator<std::pair<tvm::IterVar const, tvm::Range> > > const&, std::unordered_map<tvm::Tensor, tvm::Array<tvm::Range, void>, std::hash<tvm::Tensor>, std::equal_to<tvm::Tensor>, std::allocator<std::pair<tvm::Tensor const, tvm::Array<tvm::Range, void> > > > const&, tvm::TensorIntrin const&)+0x73c) [0x7f474b50163c]\n  [bt] (0) /home/tvm/build/libtvm.so(dmlc::LogMessageFatal::~LogMessageFatal()+0x43) [0x7f474b2e19c3]\n  File \"/home/jwfromm/tvm/src/op/tensorize.cc\", line 333\n  File \"tvm/_ffi/_cython/./function.pxi\", line 56, in tvm._ffi._cy3.core.tvm_callback\n  File \"/home/tvm/python/tvm/relay/backend/_backend.py\", line 59, in lower\n    raise RuntimeError(msg)\n  File \"/home/tvm/python/tvm/relay/backend/_backend.py\", line 51, in lower\n    f = _build.lower(sch, inputs, name=func_name)\n  File \"/home/tvm/python/tvm/build_module.py\", line 376, in lower\n    stmt = form_body(sch)\n  File \"/home/tvm/python/tvm/build_module.py\", line 326, in form_body\n    stmt = schedule.ScheduleOps(sch, bounds)\n  File \"tvm/_ffi/_cython/./function.pxi\", line 310, in tvm._ffi._cy3.core.FunctionBase.__call__\n  File \"tvm/_ffi/_cython/./function.pxi\", line 245, in tvm._ffi._cy3.core.FuncCall\n  File \"tvm/_ffi/_cython/./function.pxi\", line 234, in tvm._ffi._cy3.core.FuncCall3\n  File \"tvm/_ffi/_cython/./base.pxi\", line 171, in tvm._ffi._cy3.core.CALL\n  [bt] (7) /home/tvm/build/libtvm.so(TVMFuncCall+0x65) [0x7f474ba5ba25]\n  [bt] (6) /home/tvm/build/libtvm.so(+0x4301e9) [0x7f474b3251e9]\n  [bt] (5) /home/tvm/build/libtvm.so(tvm::schedule::ScheduleOps(tvm::Schedule, tvm::Map<tvm::IterVar, tvm::Range, void, void>, bool)+0x1354) [0x7f474b6670d4]\n  [bt] (4) /home/tvm/build/libtvm.so(tvm::schedule::MakePipeline(tvm::Stage const&, std::unordered_map<tvm::IterVar, tvm::Range, std::hash<tvm::IterVar>, std::equal_to<tvm::IterVar>, std::allocator<std::pair<tvm::IterVar const, tvm::Range> > > const&, tvm::Stmt, bool)+0x66) [0x7f474b665366]\n  [bt] (3) /home/tvm/build/libtvm.so(tvm::ComputeOpNode::BuildProvide(tvm::Stage const&, std::unordered_map<tvm::IterVar, tvm::Range, std::hash<tvm::IterVar>, std::equal_to<tvm::IterVar>, std::allocator<std::pair<tvm::IterVar const, tvm::Range> > > const&, bool) const+0x185) [0x7f474b4ce075]\n  [bt] (2) /home/tvm/build/libtvm.so(tvm::MakeTensorize(tvm::ComputeOpNode const*, tvm::Stage const&, std::unordered_map<tvm::IterVar, tvm::Range, std::hash<tvm::IterVar>, std::equal_to<tvm::IterVar>, std::allocator<std::pair<tvm::IterVar const, tvm::Range> > > const&, bool)+0x26d) [0x7f474b503c6d]\n  [bt] (1) /home/tvm/build/libtvm.so(tvm::VerifyTensorizeBody(tvm::ComputeOpNode const*, tvm::Stage const&, std::unordered_map<tvm::IterVar, tvm::Range, std::hash<tvm::IterVar>, std::equal_to<tvm::IterVar>, std::allocator<std::pair<tvm::IterVar const, tvm::Range> > > const&, std::unordered_map<tvm::Tensor, tvm::Array<tvm::Range, void>, std::hash<tvm::Tensor>, std::equal_to<tvm::Tensor>, std::allocator<std::pair<tvm::Tensor const, tvm::Array<tvm::Range, void> > > > const&, tvm::TensorIntrin const&)+0x73c) [0x7f474b50163c]\n  [bt] (0) /home/tvm/build/libtvm.so(dmlc::LogMessageFatal::~LogMessageFatal()+0x43) [0x7f474b2e19c3]\n  File \"/home/jwfromm/tvm/src/op/tensorize.cc\", line 333\nTVMError: Failed to match the data type with TensorIntrin tensor_intrin's declaration  provided=int16, intrin=uint16\nDuring handling of the above exception, another exception occurred:\n\nTVMError: Failed to match the data type with TensorIntrin tensor_intrin's declaration  provided=int16, intrin=uint16\nError during compile function\n-----------------------------\nv0.0.3\nfn (%p0: Tensor[(1, 4096), int16], %p1: Tensor[(4096, 1, 512), int8], Primitive=1) -> Tensor[(1, 4096), int16] {\n  nn.bitserial_dense(%p0, %p1, units=4096, pack_dtype=\"int8\", out_dtype=\"int16\", unipolar=False) /* ty=Tensor[(1, 4096), int16] */\n}"
     ]
    }
   ],
   "source": [
    "params = {'w': w_np}\n",
    "with relay.build_config(opt_level=0):\n",
    "    graph, lib, params = relay.build_module.build(y_func, target=target, params=params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = util.tempdir()\n",
    "lib_fname = tmp.relpath('net.tar')\n",
    "lib.export_library(lib_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "remote = autotvm.measure.request_remote(\n",
    "    'rpi3b', 'fleet.cs.washington.edu', 9190, timeout=10000)\n",
    "#remote = tvm.rpc.LocalSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# upload the library to remote device and load it\n",
    "remote.upload(lib_fname)\n",
    "rlib = remote.load_module('net.tar')\n",
    "\n",
    "# create the remote runtime module\n",
    "ctx = remote.cpu(0)\n",
    "module = runtime.create(graph, rlib, ctx)\n",
    "# set parameter (upload params to the remote device. This may take a while)\n",
    "module.set_input(**params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "module.set_input('x', x_np)\n",
    "module.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate inference time cost...\n",
      "Mean inference time (std dev): 40.56 ms (0.00 ms)\n"
     ]
    }
   ],
   "source": [
    " # Evaluate\n",
    "print(\"Evaluate inference time cost...\")\n",
    "ftimer = module.module.time_evaluator(\"run\", ctx, number=10, repeat=1)\n",
    "prof_res = np.array(ftimer().results) * 1000  # Convert to milliseconds\n",
    "print(\"Mean inference time (std dev): %.2f ms (%.2f ms)\" %\n",
    "      (np.mean(prof_res), np.std(prof_res)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
