{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Writing tunable template and Using auto-tuner\n",
    "=============================================\n",
    "**Author**: `Lianmin Zheng <https://https://github.com/merrymercy>`_\n",
    "\n",
    "This is an introduction tutorial to the auto-tuning module in tvm.\n",
    "\n",
    "There are two steps in auto-tuning.\n",
    "The first step is defining a search space.\n",
    "The second step is running a search algorithm to explore through this space.\n",
    "In this tutorial, you can learn how to perform these two steps in tvm.\n",
    "The whole workflow is illustrated by a matrix multiplication example.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install dependencies\n",
    "--------------------\n",
    "To use autotvm package in tvm, we need to install some extra dependencies.\n",
    "(change \"3\" to \"2\" if you use python2):\n",
    "\n",
    ".. code-block:: bash\n",
    "\n",
    "  pip3 install --user psutil xgboost\n",
    "\n",
    "To make tvm run faster in tuning, it is recommended to use cython\n",
    "as FFI of tvm. In the root directory of tvm, execute\n",
    "(change \"3\" to \"2\" if you use python2):\n",
    "\n",
    ".. code-block:: bash\n",
    "\n",
    "  pip3 install --user cython\n",
    "  sudo make cython3\n",
    "\n",
    "Now return to python code. Import packages.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import tvm\n",
    "\n",
    "# the module is called `autotvm`\n",
    "from tvm import autotvm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1:  Define the search space\n",
    "--------------------------------\n",
    "In this section, we will rewrite a deterministic tvm schedule code to a\n",
    "tunable schedule template. You can regard the process of search space definition\n",
    "as the parametrization of our exiting schedule code.\n",
    "\n",
    "To begin with, here is how we implement a blocked matrix multiplication in tvm.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Matmul V0: Constant tiling factor\n",
    "def matmul_v0(N, L, M, dtype):\n",
    "    A = tvm.placeholder((N, L), name='A', dtype=dtype)\n",
    "    B = tvm.placeholder((L, M), name='B', dtype=dtype)\n",
    "\n",
    "    k = tvm.reduce_axis((0, L), name='k')\n",
    "    C = tvm.compute((N, M), lambda i, j: tvm.sum(A[i, k] * B[k, j], axis=k), name='C')\n",
    "    s = tvm.create_schedule(C.op)\n",
    "\n",
    "    # schedule\n",
    "    y, x = s[C].op.axis\n",
    "    k = s[C].op.reduce_axis[0]\n",
    "\n",
    "    yo, yi = s[C].split(y, 8)\n",
    "    xo, xi = s[C].split(x, 8)\n",
    "\n",
    "    s[C].reorder(yo, xo, k, yi, xi)\n",
    "\n",
    "    return s, [A, B, C]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parametrize the schedule\n",
    "^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "In the previous schedule code, we use a constant \"8\" as tiling factor.\n",
    "However, it might not be the best one because the best tiling factor depends\n",
    "on real hardware environment and input shape.\n",
    "\n",
    "If you want the schedule code to be portable across a wider range of input shapes\n",
    "and target hardware, it is better to define a set of candidate values and\n",
    "pick the best one according to the measurement results on target hardware.\n",
    "\n",
    "In autotvm, we can define a tunable parameter, or a \"knob\" for such kind of value.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Matmul V1: List candidate values\n",
    "@autotvm.template  # 1. use a decorator\n",
    "def matmul_v1(N, L, M, dtype):\n",
    "    A = tvm.placeholder((N, L), name='A', dtype=dtype)\n",
    "    B = tvm.placeholder((L, M), name='B', dtype=dtype)\n",
    "\n",
    "    k = tvm.reduce_axis((0, L), name='k')\n",
    "    C = tvm.compute((N, M), lambda i, j: tvm.sum(A[i, k] * B[k, j], axis=k), name='C')\n",
    "    s = tvm.create_schedule(C.op)\n",
    "\n",
    "    # schedule\n",
    "    y, x = s[C].op.axis\n",
    "    k = s[C].op.reduce_axis[0]\n",
    "\n",
    "    # 2. get the config object\n",
    "    cfg = autotvm.get_config()\n",
    "\n",
    "    # 3. define search space\n",
    "    cfg.define_knob(\"tile_y\", [1, 2, 4, 8, 16])\n",
    "    cfg.define_knob(\"tile_x\", [1, 2, 4, 8, 16])\n",
    "\n",
    "    # 4. schedule according to config\n",
    "    yo, yi = s[C].split(y, cfg['tile_y'].val)\n",
    "    xo, xi = s[C].split(x, cfg['tile_x'].val)\n",
    "\n",
    "    s[C].reorder(yo, xo, k, yi, xi)\n",
    "\n",
    "    return s, [A, B, C]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we make four modifications to the previous schedule code and get\n",
    "a tunable \"template\". We can explain the modifications one by one.\n",
    "\n",
    "1. Use a decorator to mark this function as a simple template\n",
    "2. Get a config object:\n",
    "   You can regard this :code:`cfg` as an argument of this function but\n",
    "   we obtain it in a different way. With this argument, this function is no longer\n",
    "   a deterministic schedule code. Instead, we can pass different configurations to\n",
    "   this function and get different schedules, so this function is a \"template\".\n",
    "\n",
    "   To make the template function more compact, we do two things in a single function.\n",
    "   (1) define a search space and (2) schedule according to an entity in this space.\n",
    "   To achieve this, we make :code:`cfg` be either\n",
    "   a :any:`ConfigSpace` or a :any:`ConfigEntity` object.\n",
    "\n",
    "   When it is a :any:`ConfigSpace`, it will collect all tunable knobs in this function and\n",
    "   build the search space.\n",
    "   When it is a :any:`ConfigEntity`, it will ignore all space definition API\n",
    "   (namely, :code:`cfg.define_XXXXX(...)`).   Instead, it stores deterministic values for\n",
    "   all tunable knobs, and we schedule according to these values.\n",
    "\n",
    "   During auto-tuning, we will first call this template with a :any:`ConfigSpace`\n",
    "   object to build the search space. Then we call this template with different :any:`ConfigEntity`\n",
    "   in the built space to get different schedules. Finally we will measure the code generated by\n",
    "   different schedules and pick the best one.\n",
    "\n",
    "3. Define two tunable knobs. The first one is :code:`tile_y` with\n",
    "   5 possible values. The second one is :code:`tile_x` with a same\n",
    "   list of possible values. These two knobs are independent, so they\n",
    "   span a search space with size = 5x5 = 25\n",
    "4. Schedule according to the deterministic values in :code:`cfg`\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use better space definition API\n",
    "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "In the previous template, we manually list all possible values for a knob.\n",
    "This is the lowest level API to define the space.\n",
    "However, we also provide another set of API to make the space definition\n",
    "easier and smarter. It is recommended to use this set of high level API.\n",
    "\n",
    "In the flowing example, we use :any:`ConfigSpace.define_split` to define a split\n",
    "knob. It will enumerate all the possible ways to split an axis and construct\n",
    "the space.\n",
    "\n",
    "We also have :any:`ConfigSpace.define_reorder` for reorder knob and\n",
    ":any:`ConfigSpace.define_annotate` for annotation like unroll, vectorization,\n",
    "thread binding.\n",
    "When the high level API cannot meet your requirement, you can always fall\n",
    "back to use low level API.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "@autotvm.template\n",
    "def matmul(N, L, M, dtype):\n",
    "    A = tvm.placeholder((N, L), name='A', dtype=dtype)\n",
    "    B = tvm.placeholder((L, M), name='B', dtype=dtype)\n",
    "\n",
    "    k = tvm.reduce_axis((0, L), name='k')\n",
    "    C = tvm.compute((N, M), lambda i, j: tvm.sum(A[i, k] * B[k, j], axis=k), name='C')\n",
    "    s = tvm.create_schedule(C.op)\n",
    "\n",
    "    # schedule\n",
    "    y, x = s[C].op.axis\n",
    "    k = s[C].op.reduce_axis[0]\n",
    "\n",
    "    ##### define space begin #####\n",
    "    cfg = autotvm.get_config()\n",
    "    cfg.define_split(\"tile_y\", y, num_outputs=2)\n",
    "    cfg.define_split(\"tile_x\", x, num_outputs=2)\n",
    "    ##### define space end #####\n",
    "\n",
    "    # schedule according to config\n",
    "    yo, yi = cfg[\"tile_y\"].apply(s, C, y)\n",
    "    xo, xi = cfg[\"tile_x\"].apply(s, C, x)\n",
    "\n",
    "    s[C].reorder(yo, xo, k, yi, xi)\n",
    "\n",
    "    return s, [A, B, C]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\"><h4>Note</h4><p>More Explanation on :code:`cfg.defile_split`</p></div>\n",
    "\n",
    " In this template, :code:`cfg.define_split(\"tile_y\", y, num_outputs=2)` will enumerate\n",
    " all possible combinations that can split axis y into two axes with factors of the length of y.\n",
    " For example, if the length of y is 32 and we want to split it into two axes\n",
    " using factors of 32, then there are 6 possible values for\n",
    " (length of outer axis, length of inner axis) pair, namely\n",
    " (32, 1), (16, 2), (8, 4), (4, 8), (2, 16) or (1, 32).\n",
    " They are just the 6 possible values of `tile_y`.\n",
    "\n",
    " During schedule, :code:`cfg[\"tile_y\"]` is a :code:`SplitEntity` object.\n",
    " We stores the lengths of outer axes and inner axes in :code:`cfg['tile_y'].size`\n",
    " (a tuple with two elements).\n",
    " In this template, we apply it by using :code:`yo, yi = cfg['tile_y'].apply(s, C, y)`.\n",
    " Actually, this is equivalent to\n",
    " :code:`yo, yi = s[C].split(y, cfg[\"tile_y\"].size[1])`\n",
    " or  :code:`yo, yi = s[C].split(y, nparts=cfg['tile_y\"].size[0])`\n",
    "\n",
    " The advantage of using cfg.apply API is that it makes multi-level split\n",
    " (when num_outputs >= 3) easier.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2:  Search through the space\n",
    "---------------------------------\n",
    "In step 1, we build the search space by extending our old schedule code\n",
    "into a template. The next step is to pick a tuner and explore in this space.\n",
    "\n",
    "Auto-tuners in tvm\n",
    "^^^^^^^^^^^^^^^^^^\n",
    "The job for a tuner can be described by following pseudo code\n",
    "\n",
    "  .. code-block:: c\n",
    "\n",
    "   ct = 0\n",
    "   while ct < max_number_of_trials:\n",
    "       propose a batch of configs\n",
    "       measure this batch of configs on real hardware and get results\n",
    "       ct += batch_size\n",
    "\n",
    "When proposing the next batch of configs, the tuner can take different strategies. We\n",
    "provide four tuners with different strategies in autotvm.\n",
    "\n",
    "* :any:`RandomTuner`: Enumerate the space in a random order\n",
    "* :any:`GridSearchTuner`: Enumerate the space in a grid search order\n",
    "* :any:`GATuner`: Using genetic algorithm to search through the space\n",
    "* :any:`XGBTuner`: Uses a model based method. Train a XGBoost model to predict the speed of lowered IR and pick the next batch according to the prediction.\n",
    "\n",
    "You can choose the tuner according to the size of your space, your time budget and other factors.\n",
    "For example, if your space is very small (less than 1000), a gridsearch tuner or a\n",
    "random tuner is good enough. If your space is at the level of 10^9 (this is the space\n",
    "size of a conv2d operator on CUDA GPU), XGBoostTuner can explore more efficiently\n",
    "and find better configs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Begin tuning\n",
    "^^^^^^^^^^^^\n",
    "Here we continue our matrix multiplication example.\n",
    "First we should create a tuning task.\n",
    "We can also inspect the initialized search space.\n",
    "In this case, for a 512x512 square matrix multiplication, the space size\n",
    "is 10x10=100\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConfigSpace (len=100, space_map=\n",
      "   0 tile_y: Split(policy=all, product=512, num_outputs=2) len=10\n",
      "   1 tile_x: Split(policy=all, product=512, num_outputs=2) len=10\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "N, L, M = 512, 512, 512\n",
    "task = autotvm.task.create(matmul, args=(N, L, M, 'float32'), target='llvm')\n",
    "print(task.config_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we need to define how to measure the generated code and pick a tuner.\n",
    "Since our space is small, a random tuner is just okay.\n",
    "\n",
    "We only make 10 trials in this tutorial for demonstration. In practice,\n",
    "you can do more trials according to your time budget.\n",
    "We will log the tuning results into a log file. This file can be\n",
    "used to get the best config later.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-1:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/jwfromm/tvm/python/tvm/rpc/tracker.py\", line 339, in _tracker_server\n",
      "    handler.run()\n",
      "  File \"/home/jwfromm/tvm/python/tvm/rpc/tracker.py\", line 335, in run\n",
      "    self._ioloop.start()\n",
      "  File \"/home/jwfromm/.local/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 132, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/usr/lib/python3.6/asyncio/base_events.py\", line 414, in run_forever\n",
      "    raise RuntimeError('This event loop is already running')\n",
      "RuntimeError: This event loop is already running\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Cannot get remote devices from the tracker. Please check the status of tracker by 'python -m tvm.exec.query_rpc_tracker --port [THE PORT YOU USE]' and make sure you have free devices on the queue status.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-9def4ed475dc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m tuner.tune(n_trial=10,\n\u001b[1;32m     15\u001b[0m            \u001b[0mmeasure_option\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmeasure_option\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m            callbacks=[autotvm.callback.log_to_file('matmul.log')])\n\u001b[0m",
      "\u001b[0;32m~/tvm/python/tvm/autotvm/tuner/tuner.py\u001b[0m in \u001b[0;36mtune\u001b[0;34m(self, n_trial, measure_option, early_stopping, callbacks)\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0mevery\u001b[0m \u001b[0mmeasurement\u001b[0m \u001b[0mpair\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mSee\u001b[0m \u001b[0mautotvm\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mtuner\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpy\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msome\u001b[0m \u001b[0mexamples\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \"\"\"\n\u001b[0;32m---> 92\u001b[0;31m         \u001b[0mmeasure_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_measure_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeasure_option\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m         \u001b[0mn_parallel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmeasure_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'n_parallel'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0mearly_stopping\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mearly_stopping\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;36m1e9\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tvm/python/tvm/autotvm/measure/measure.py\u001b[0m in \u001b[0;36mcreate_measure_batch\u001b[0;34m(task, option)\u001b[0m\n\u001b[1;32m    234\u001b[0m     \u001b[0mrunner\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moption\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'runner'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 236\u001b[0;31m     \u001b[0mattach_objects\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrunner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_task\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m     \u001b[0;31m# feed device related information from runner to builder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tvm/python/tvm/autotvm/measure/measure_methods.py\u001b[0m in \u001b[0;36mset_task\u001b[0;34m(self, task)\u001b[0m\n\u001b[1;32m    324\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mport\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtracker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLocalRunner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_task\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    327\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mserver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtracker\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tvm/python/tvm/autotvm/measure/measure_methods.py\u001b[0m in \u001b[0;36mset_task\u001b[0;34m(self, task)\u001b[0m\n\u001b[1;32m    194\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Get devices for measurement successfully!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m             raise RuntimeError(\"Cannot get remote devices from the tracker. \"\n\u001b[0m\u001b[1;32m    197\u001b[0m                                \u001b[0;34m\"Please check the status of tracker by \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                                \u001b[0;34m\"'python -m tvm.exec.query_rpc_tracker --port [THE PORT YOU USE]' \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Cannot get remote devices from the tracker. Please check the status of tracker by 'python -m tvm.exec.query_rpc_tracker --port [THE PORT YOU USE]' and make sure you have free devices on the queue status."
     ]
    }
   ],
   "source": [
    "# logging config (for printing tuning log to the screen)\n",
    "logging.getLogger('autotvm').setLevel(logging.DEBUG)\n",
    "logging.getLogger('autotvm').addHandler(logging.StreamHandler(sys.stdout))\n",
    "\n",
    "# There are two steps for measuring a config: build and run.\n",
    "# By default, we use all cpu cores to compile program. Then measure them sequentially.\n",
    "# We measure 5 times and take average to reduce variance.\n",
    "measure_option = autotvm.measure_option(\n",
    "    builder='local',\n",
    "    runner=autotvm.LocalRunner(number=5))\n",
    "\n",
    "# begin tuning, log records to file `matmul.log`\n",
    "tuner = autotvm.tuner.RandomTuner(task)\n",
    "tuner.tune(n_trial=10,\n",
    "           measure_option=measure_option,\n",
    "           callbacks=[autotvm.callback.log_to_file('matmul.log')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we apply history best from the cache file and check its correctness.\n",
    "We can call the function :code:`matmul` directly under the \n",
    ":any:`autotvm.apply_history_best` context. When we call this function,\n",
    "it will query the dispatch context with its argument and get the best config \n",
    "with the same argument.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# apply history best from log file\n",
    "with autotvm.apply_history_best('matmul.log'):\n",
    "    with tvm.target.create(\"llvm\"):\n",
    "        s, arg_bufs = matmul(N, L, M, 'float32')\n",
    "        func = tvm.build(s, arg_bufs)\n",
    "\n",
    "# check correctness\n",
    "a_np = np.random.uniform(size=(N, L)).astype(np.float32)\n",
    "b_np = np.random.uniform(size=(L, M)).astype(np.float32)\n",
    "c_np = a_np.dot(b_np)\n",
    "\n",
    "c_tvm = tvm.nd.empty(c_np.shape)\n",
    "func(tvm.nd.array(a_np), tvm.nd.array(b_np), c_tvm)\n",
    "\n",
    "tvm.testing.assert_allclose(c_np, c_tvm.asnumpy(), rtol=1e-2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
